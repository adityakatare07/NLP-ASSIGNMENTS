{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2d2b3e",
   "metadata": {},
   "source": [
    "## TOKENIZER OUTPUT OF QUESTION 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aab8a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer_outputs = [\n",
    "    [\"जब आप\", \"अपने लक्षित खोजशब्दों\", \"की पहचान कर सकते हैं\", \"और\", \"अमेज़ॅन\", \"खोज परिणाम पृष्ठ पर\", \"आपके उत्पाद\", \"कैसे प्रदर्शन कर रहे हैं\", \"तो आप\", \"अपने लिस्टिंग अनुकूलन को\", \"बेहतर बनाने\", \"और\", \"मौजूदा त्रुटियों को ठीक करने में\", \"सक्षम होंगे\"],\n",
    "    [\"तहसील स्तर पर\", \"रायपुर तहसील के लिए\", \"बाढ़ नियंत्रण प्रभारी\", \"तहसीलदार उमेश साहू\", \"मोबाइल नंबर\", \"83053-77283\", \"को\", \"प्रभारी अधिकारी\", \"तथा नियंत्रण कक्ष का\", \"दूरभाष नंबर\", \"0771-2224163\", \"है\"],\n",
    "    [\"इसका मतलब है कि\", \"स्कूल\", \"बच्चों की फीस\", \"से साला\", \"करीब 150 करोड़ रुपये\", \"कमा रहे हैं\"],\n",
    "    [\"वास्तव में\", \"कभी-कभी\", \"हमारे जीवन में\", \"संघर्ष ही\", \"वो चीज होती\", \"जिसकी हमें सचमुच आवश्यकता होती है\", \"यदि\", \"हम\", \"बिना किसी\", \"struggle के सब कुछ\", \"पाने लगे तो\", \"हम भी\", \"एक अपंग के\"],\n",
    "    [\"शिवम दुबे को\", \"इस मैच में\", \"तीसरे नंबर पर\", \"बल्लेबाजी के लिए\", \"भेजा गया\"],\n",
    "    [\"आज की तारीख है\", \"10 अगस्त 2018\", \"तथा आज इस वर्ष का\", \"222वां दिन है\", \"इन 222 दिनों में\", \"प्रारंग अपने परिवार के जौनपुरवासियों तक\", \"208 लेख पहुंचा चुका है\", \"तथा यह लेख 209वां लेख होगा\"],\n",
    "    [\"उसके कंधे में\", \"दर्द\", \"शुरू हो गया\"],\n",
    "    [\"जिससे उनके सफर में\", \"करीब 20 मिनट की\", \"बचत होगी\"],\n",
    "    [\"पुलिस ने स्पष्ट कहा था\", \"कि\", \"अगर\", \"मुठभेड़ के दौरान\", \"कोई स्थानीय आतंकवादी आत्मसमर्पण करने की गुजारिश करता है\", \"तो\", \"उसे स्वीकार लिया जाएगा\"],\n",
    "    [\"प्रदेश में\", \"1 लाख 73 हजार 187 व्यक्तियों पर\", \"प्रतिबन्धात्मक कार्यवाही मध्यप्रदेश\", \"1 लाख 60 हजार डाक मतपत्र का उपयोग\"],\n",
    "    [\"यही तो\", \"सत्यवती ने\", \"चाहा था\"],\n",
    "    [\"'NDTV युवा'\", \"अभिषेक बच्चन ने\", \"कहा\", \"देश में\", \"टैलेंट की कमी नहीं\", \"उसे स्पॉटलाइट में ले जाने की ज़रूरत\"],\n",
    "    [\"2014 के चुनावों में\", \"जनता ने\", \"जब\", \"सब कुछ\", \"कीचड़ कर दिया\", \"और\", \"पूरे देश में\", \"कमल खिला दिया\", \"तो\", \"वे समझ गए\", \"कि\", \"केंद्र में\", \"उनके पास\", \"तो\", \"क्या\", \"पूरे विपक्ष के पास\", \"आज करने के लिए\", \"कुछ ख़ास नहीं है\", \"तो\", \"वापस प्रदेश में\", \"ध्यान लगाया\", \"लेकिन\", \"अब तक\", \"लगभग\", \"दो साल\", \"बीत चुके थे\", \"और\", \"समय के साथ\", \"उन्हीं की दी\", \"साइकिल पर\", \"बैठ कर\", \"बेटा काफी आगे निकल चुका था\", \"जबकि\", \"वो प्रदेश से निकल कर केंद्र में जाने की चाह में\", \"बहुत पीछे जा चुके थे\", \"हवा के बहाव में\", \"वे वहाँ भी खड़े नहीं रह पाए\", \"जहाँ पर वह थे\"],\n",
    "    [\"चार महानगरों और\", \"बेंगलूर व हैदराबाद\", \"के\", \"हवाई अड्डों से ही\", \"70 प्रतिशत\", \"हवाई यातायात परिवहन का परिचालन होता है\"],\n",
    "    [\"इस संदर्भ में\", \"कोई पक्का साक्ष्य\", \"नहीं मिला है\"],\n",
    "    [\"होंडा मोटरसाइकिल एंड स्कूटर\", \"लेकर आये है\", \"एक बेहद ख़ास टू-व्हीलर\", \"जिसका नाम है ‘NAVi’\", \"यह एक अलग टाइप का टू-व्हीलर है\"],\n",
    "    [\"लीग के सीजन-5 के\", \"मैचों की टीआरपी ने\", \"हाल ही में\", \"भारत और श्रीलंका के बीच\", \"हुई टेस्ट सीरीज\", \"के दर्शकों की संख्या को\", \"पीछे कर दिया\"],\n",
    "    [\"चीन के साथ संघर्ष के कुछ ही समय बाद\", \"नेहरू के स्वास्थ्य में\", \"गिरावट के लक्षण\", \"दिखाई देने लगे\", \"उन्हें 1963 में\", \"दिल का हल्का दौरा पड़ा\", \"जनवरी 1964 में\", \"उन्हें और दुर्बल बना देने वाला दौरा पड़ा\", \"कुछ ही महीनों के बाद\", \"तीसरे दौरे में 27 मई 1964 में\", \"उनकी मृत्यु हो गई\"],\n",
    "    [\"आम जिंदगी\", \"एक पड़ाव पर आकर\", \"ऐसा मोड़ लेती है\", \"जिसकी\", \"आप कल्पना भी नहीं कर सकते हैं\", \"बधाई हो\", \"जिंदगी के उसी मोड़ की\", \"कहानी है\", \"जो किसी के साथ भी घट सकती है\"],\n",
    "    [\"त्यानंतर\", \"झेडपी प्रशासनाने\", \"या विभागाचा कार्यभार\", \"सामान्य प्रशासन विभागाचे उपमुख्य कार्यकारी अधिकारी\", \"सत्यजीत बडे यांच्याकडे\", \"देण्यात आला होता\"],\n",
    "    [\"जिसके पक्ष में\", \"कोई आधिकारिक आंकड़े\", \"तो नहीं हैं\", \"लेकिन\", \"लोक अवधारणा\", \"इस विचार की\", \"पुष्टि करती है\"],\n",
    "    [\"आल्पस कंपनी\", \"के बाहर धरने पर बैठे\", \"14० कर्मचारियों ने\", \"अपनी मांगों को लेकर\", \"जमकर नारेबाजी कर\", \"धरना प्रदर्शन किया\"],\n",
    "    [\"इस में\", \"कुछ भी\", \"मुश्किल नहीं है\", \"आपको\", \"अपने संभावित सहयोगी के साथ\", \"अग्रिम में\", \"चर्चा करने की\", \"ज़रूरत है\", \"यही है\", \"सौदा के लिए\", \"समय सीमा\"],\n",
    "    [\"अक्तूबर में\", \"आशाओं में\", \"सफलता\", \"धन-लाभ\", \"लेखन\", \"विद्यार्थी कंपीटिशन वालों को लाभ\", \"शत्रु पर विजय\", \"भागदौड़\", \"अपव्यय\", \"कार्य विलंब\", \"किसी संबंधी को कष्ट\"],\n",
    "    [\"1998 मार्को एबी सक्सोन लिफ्टों\", \"इंग्लैंड के मालिकों में से\", \"एक बन गया\", \"सक्सोन लिफ्टों\", \"इंग्लैंड में\", \"कस्टम निर्मित कैंची उठाने तालिकाओं\", \"के अग्रणी निर्माता में से\", \"एक है\"]\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b88beee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_vowels = ['अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ए', 'ऐ', 'ओ', 'औ', 'अं', 'अः', 'अँ']\n",
    "hindi_matras = ['ा', 'ि', 'ी', 'ु', 'ू', 'े', 'ै', 'ो', 'ौ', 'ं', 'ः', 'ँ']\n",
    "Corresponding_vowel_dic = {'ा': 'आ', 'ि': 'इ', 'ी': 'ई', 'ु': 'उ', 'ू': 'ऊ', 'े': 'ए', 'ै': 'ऐ', 'ो': 'ओ', 'ौ': 'औ', 'ं': 'अं', 'ः': 'अः', 'ँ': 'अँ'}\n",
    "Corresponding_matras_dic = {'आ': 'ा', 'इ': 'ि', 'ई': 'ी', 'उ': 'ु', 'ऊ': 'ू', 'ए': 'े', 'ऐ': 'ै', 'ओ': 'ो', 'औ': 'ौ', 'अं': 'ं', 'अः': 'ः', 'अँ': 'ँ'}\n",
    "halant = '\\u094D'\n",
    "aa = 'अ'\n",
    "hindi_punctuation_symbols = \"##▁#।▁,?!.:;‘’“”-…()▁ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "alphabet_string = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfd623a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hindi_symbol_prefix(x):\n",
    "    if x[0] in hindi_punctuation_symbols:\n",
    "        return x[1:]\n",
    "    else:\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d35748a",
   "metadata": {},
   "source": [
    "## FUNCTION TO CALCULATE PRECISION RECALL AND FSCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3caa877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(tokenizer_outputs, brr):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    for i in range(len(tokenizer_outputs)):\n",
    "        for token in brr[i]:\n",
    "            if token in tokenizer_outputs[i]:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "        for token in tokenizer_outputs[i]:\n",
    "            if token not in brr[i]:\n",
    "                false_negatives += 1\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "\n",
    "    # Calculate recall\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "\n",
    "    # Calculate F-score\n",
    "    fscore = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, fscore\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af41dfb4",
   "metadata": {},
   "source": [
    "## INPUT SENTENCE OF QUESTION 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e13bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    [\"जब आप अपने लक्षित खोजशब्दों की पहचान कर सकते हैं और अमेज़ॅन खोज परिणाम पृष्ठ पर आपके उत्पाद कैसे प्रदर्शन कर रहे हैं, तो आप अपने लिस्टिंग अनुकूलन को बेहतर बनाने और मौजूदा त्रुटियों को ठीक करने में सक्षम होंगे।\"],\n",
    "    [\"तहसील स्तर पर रायपुर तहसील के लिए बाढ़ नियंत्रण प्रभारी तहसीलदार उमेश साहू मोबाइल नंबर 83053-77283 को प्रभारी अधिकारी तथा नियंत्रण कक्ष का दूरभाष नंबर 0771-2224163 है\"],\n",
    "    [\"इसका मतलब है कि स्कूल बच्चों की फीस से साला करीब 150 करोड़ रुपये कमा रहे हैं।\"],\n",
    "    [\"वास्तव में कभी-कभी हमारे जीवन में संघर्ष ही वो चीज होती जिसकी हमें सचमुच आवश्यकता होती है। यदि हम बिना किसी struggle के सब कुछ पाने लगे तो हम भी एक अपंग के सामान हो जायेंगे।\"],\n",
    "    [\"शिवम दुबे को इस मैच में तीसरे नंबर पर बल्लेबाजी के लिए भेजा गया।\"],\n",
    "    [\"आज की तारीख है 10 अगस्त 2018 तथा आज इस वर्ष का 222वां दिन है। इन 222 दिनों में प्रारंग अपने परिवार के जौनपुरवासियों तक 208 लेख पहुंचा चुका है तथा यह लेख 209वां लेख होगा।\"],\n",
    "    [\"उसके कंधे में दर्द शुरू हो गया।\"],\n",
    "    [\"जिससे उनके सफर में करीब 20 मिनट की बचत होगी।\"],\n",
    "    [\"पुलिस ने स्पष्ट कहा था कि अगर मुठभेड़ के दौरान कोई स्थानीय आतंकवादी आत्मसमर्पण करने की गुजारिश करता है तो उसे स्वीकार लिया जाएगा।\"],\n",
    "    [\"प्रदेश में 1 लाख 73 हजार 187 व्यक्तियों पर प्रतिबन्धात्मक कार्यवाही—-मध्यप्रदेश— 1 लाख 60 हजार डाक-मतपत्र का उपयोग\"],\n",
    "    [\"यही तो सत्यवती ने चाहा था।\"],\n",
    "    [\"'NDTV युवा': अभिषेक बच्चन ने कहा, देश में टैलेंट की कमी नहीं, उसे स्पॉटलाइट में ले जाने की ज़रूरत\"],\n",
    "    [\"2014 के चुनावों में जनता ने जब सब कुछ कीचड़ कर दिया और पूरे देश में कमल खिला दिया तो वे समझ गए कि केंद्र में उनके पास तो क्या पूरे विपक्ष के पास आज करने के लिए कुछ ख़ास नहीं है तो वापस प्रदेश में ध्यान लगाया लेकिन अब तक लगभग दो साल बीत चुके थे और समय के साथ उन्हीं की दी साइकिल पर बैठ कर बेटा काफी आगे निकल चुका था। जबकि वो प्रदेश से निकल कर केंद्र में जाने की चाह में बहुत पीछे जा चुके थे, हवा के बहाव में वे वहाँ भी खड़े नहीं रह पाए जहाँ पर वह थे।\"],\n",
    "    [\"चार महानगरों और बेंगलूर व हैदराबाद के हवाई अड्डों से ही 70 प्रतिशत हवाई यातायात परिवहन का परिचालन होता है।\"],\n",
    "    [\"इस संदर्भ में कोई पक्का साक्ष्य नहीं मिला है।\"],\n",
    "    [\"होंडा मोटरसाइकिल एंड स्कूटर लेकर आये है एक बेहद ख़ास टू-व्हीलर जिसका नाम है ‘NAVi’, यह एक अलग टाइप का टू-व्हीलर है।\"],\n",
    "    [\"लीग के सीजन-5 के मैचों की टीआरपी ने हाल ही में भारत और श्रीलंका के बीच हुई टेस्ट सीरीज के दर्शकों की संख्या को पीछे कर दिया।\"],\n",
    "    [\"चीन के साथ संघर्ष के कुछ ही समय बाद नेहरू के स्वास्थ्य में गिरावट के लक्षण दिखाई देने लगे। उन्हें 1963 में दिल का हल्का दौरा पड़ा, जनवरी 1964 में उन्हें और दुर्बल बना देने वाला दौरा पड़ा। कुछ ही महीनों के बाद तीसरे दौरे में 27 मई 1964 में उनकी मृत्यु हो गई।\"],\n",
    "    [\"आम जिंदगी एक पड़ाव पर आकर ऐसा मोड़ लेती है जिसकी आप कल्पना भी नहीं कर सकते हैं। बधाई हो जिंदगी के उसी मोड़ की कहानी है जो किसी के साथ भी घट सकती है।\"],\n",
    "    [\"त्यानंतर झेडपी प्रशासनाने या विभागाचा कार्यभार सामान्य प्रशासन विभागाचे उपमुख्य कार्यकारी अधिकारी सत्यजीत बडे यांच्याकडे देण्यात आला होता।\"],\n",
    "    [\"जिसके पक्ष में कोई आधिकारिक आंकड़े तो नहीं हैं, लेकिन लोक अवधारणा इस विचार की पुष्टि करती है।\"],\n",
    "    [\"आल्पस कंपनी के बाहर धरने पर बैठे 14० कर्मचारियों ने अपनी मांगों को लेकर जमकर नारेबाजी कर धरना प्रदर्शन किया।\"],\n",
    "    [\"इस में कुछ भी मुश्किल नहीं है, आपको अपने संभावित सहयोगी के साथ अग्रिम में चर्चा करने की ज़रूरत है, यही है, सौदा के लिए समय सीमा।\"],\n",
    "    [\"अक्तूबर में आशाओं में सफलता, धन-लाभ, लेखन, विद्यार्थी कंपीटिशन वालों को लाभ, शत्रु पर विजय, भागदौड़, अपव्यय, कार्य विलंब, किसी संबंधी को कष्ट।\"],\n",
    "    [\"1998 मार्को एबी सक्सोन लिफ्टों, इंग्लैंड के मालिकों में से एक बन गया । सक्सोन लिफ्टों इंग्लैंड में कस्टम निर्मित कैंची उठाने तालिकाओं के अग्रणी निर्माता में से एक है ।\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe48455",
   "metadata": {},
   "source": [
    "## PRECISION RECALL AND FSCORE FOR UNIGRAM MODEL VOCAB SIZE =1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6146d32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "corpus_path = \"corpus .txt\"  # Path to your corpus file\n",
    "\n",
    "spm.SentencePieceTrainer.train(input=corpus_path, model_prefix=\"unigram_model1\", model_type=\"unigram\",vocab_size=1000)\n",
    "\n",
    "tokenizer = spm.SentencePieceProcessor(model_file=\"unigram_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c6718fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brr = []\n",
    "\n",
    "for j in range(0, len(sentences)):\n",
    "    sentence = sentences[j][0]\n",
    "    tokens = tokenizer.encode(sentence)\n",
    "\n",
    "    # Decode tokens back to text\n",
    "    decoded_sentence = tokenizer.decode(tokens)\n",
    "\n",
    "    # Print tokens separately\n",
    "    arr = []\n",
    "    for token in tokens:\n",
    "        if tokenizer.id_to_piece(token) not in hindi_punctuation_symbols:\n",
    "            x = remove_hindi_symbol_prefix(tokenizer.id_to_piece(token))\n",
    "            arr.append(x)\n",
    "    brr.append(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea91bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hash(string):\n",
    "    # Check if the string starts with '#'\n",
    "    if string.startswith('#'):\n",
    "        # Remove the leading '#' characters\n",
    "        return string.lstrip('#')\n",
    "    else:\n",
    "        # If no leading '#' characters, return the string as is\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff5fce6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECISION: 0.038461538461538464\n",
      "RECALL: 0.14084507042253522\n",
      "FSCORE: 0.06042296072507553\n"
     ]
    }
   ],
   "source": [
    "pre,rec,fsc=calculate_metrics(tokenizer_outputs,brr)\n",
    "\n",
    "print(\"PRECISION:\",pre)\n",
    "print(\"RECALL:\",rec)\n",
    "print(\"FSCORE:\",fsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7abc97",
   "metadata": {},
   "source": [
    "## PRECISION RECALL AND FSCORE FOR UNIGRAM MODEL VOCAB SIZE =2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73072869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "corpus_path = \"corpus .txt\" \n",
    "spm.SentencePieceTrainer.train(input=corpus_path, model_prefix=\"unigram_model2\", model_type=\"unigram\",vocab_size=2000)\n",
    "tokenizer = spm.SentencePieceProcessor(model_file=\"unigram_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77778670",
   "metadata": {},
   "outputs": [],
   "source": [
    "brr = []\n",
    "\n",
    "for j in range(0, len(sentences)):\n",
    "    sentence = sentences[j][0]\n",
    "    tokens = tokenizer.encode(sentence)\n",
    "\n",
    "    # Decode tokens back to text\n",
    "    decoded_sentence = tokenizer.decode(tokens)\n",
    "\n",
    "    # Print tokens separately\n",
    "    arr = []\n",
    "    for token in tokens:\n",
    "        if tokenizer.id_to_piece(token) not in hindi_punctuation_symbols:\n",
    "            x = remove_hindi_symbol_prefix(tokenizer.id_to_piece(token))\n",
    "            arr.append(x)\n",
    "    brr.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10669d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECISION: 0.038461538461538464\n",
      "RECALL: 0.14084507042253522\n",
      "FSCORE: 0.06042296072507553\n"
     ]
    }
   ],
   "source": [
    "pre,rec,fsc=calculate_metrics(tokenizer_outputs,brr)\n",
    "\n",
    "print(\"PRECISION:\",pre)\n",
    "print(\"RECALL:\",rec)\n",
    "print(\"FSCORE:\",fsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dc167d",
   "metadata": {},
   "source": [
    "## PRECISION RECALL AND FSCORE FOR BPE MODEL VOCAB SIZE =1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22893eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "corpus_path = \"corpus .txt\" \n",
    "spm.SentencePieceTrainer.train(input=corpus_path, model_prefix=\"bpe_model\", model_type=\"bpe\",vocab_size=1000)\n",
    "tokenizer = spm.SentencePieceProcessor(model_file=\"bpe_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb0aed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "brr = []\n",
    "\n",
    "for j in range(0, len(sentences)):\n",
    "    sentence = sentences[j][0]\n",
    "    tokens = tokenizer.encode(sentence)\n",
    "\n",
    "    # Decode tokens back to text\n",
    "    decoded_sentence = tokenizer.decode(tokens)\n",
    "\n",
    "    # Print tokens separately\n",
    "    arr = []\n",
    "    for token in tokens:\n",
    "        if tokenizer.id_to_piece(token) not in hindi_punctuation_symbols:\n",
    "            x = remove_hindi_symbol_prefix(tokenizer.id_to_piece(token))\n",
    "            arr.append(x)\n",
    "    brr.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f951ba43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECISION: 0.019952114924181964\n",
      "RECALL: 0.11627906976744186\n",
      "FSCORE: 0.0340599455040872\n"
     ]
    }
   ],
   "source": [
    "pre,rec,fsc=calculate_metrics(tokenizer_outputs,brr)\n",
    "\n",
    "print(\"PRECISION:\",pre)\n",
    "print(\"RECALL:\",rec)\n",
    "print(\"FSCORE:\",fsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd364516",
   "metadata": {},
   "source": [
    "## PRECISION RECALL AND FSCORE FOR BPE MODEL VOCAB SIZE=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c756222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "corpus_path = \"corpus .txt\" \n",
    "spm.SentencePieceTrainer.train(input=corpus_path, model_prefix=\"bpe_model\", model_type=\"bpe\",vocab_size=2000)\n",
    "tokenizer = spm.SentencePieceProcessor(model_file=\"bpe_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31fdc3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "brr = []\n",
    "\n",
    "for j in range(0, len(sentences)):\n",
    "    sentence = sentences[j][0]\n",
    "    tokens = tokenizer.encode(sentence)\n",
    "\n",
    "    # Decode tokens back to text\n",
    "    decoded_sentence = tokenizer.decode(tokens)\n",
    "\n",
    "    # Print tokens separately\n",
    "    arr = []\n",
    "    for token in tokens:\n",
    "        if tokenizer.id_to_piece(token) not in hindi_punctuation_symbols:\n",
    "            x = remove_hindi_symbol_prefix(tokenizer.id_to_piece(token))\n",
    "            arr.append(x)\n",
    "    brr.append(arr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eec274b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECISION: 0.024644549763033177\n",
      "RECALL: 0.12206572769953052\n",
      "FSCORE: 0.04100946372239748\n"
     ]
    }
   ],
   "source": [
    "pre,rec,fsc=calculate_metrics(tokenizer_outputs,brr)\n",
    "\n",
    "print(\"PRECISION:\",pre)\n",
    "print(\"RECALL:\",rec)\n",
    "print(\"FSCORE:\",fsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8af7cc",
   "metadata": {},
   "source": [
    "## PRECISION RECALL AND FSCORE FOR INDICBERT MODEL  MAX LENGTH =1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7dc4536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\",max_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69d2c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "brr = []\n",
    "\n",
    "for j in range(0, len(sentences)):\n",
    "    sentence = sentences[j][0]\n",
    "    tokens = tokenizer.encode(sentence)\n",
    "\n",
    "    arr = []\n",
    "    for token_id in tokens:\n",
    "        token = tokenizer.decode([token_id])  # Decode token ID to get the actual token\n",
    "        if token not in hindi_punctuation_symbols:\n",
    "            x = remove_hindi_symbol_prefix(token)\n",
    "            arr.append(x)\n",
    "    brr.append(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "49449be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECISION: 0.011131725417439703\n",
      "RECALL: 0.05555555555555555\n",
      "FSCORE: 0.01854714064914992\n"
     ]
    }
   ],
   "source": [
    "pre,rec,fsc=calculate_metrics(tokenizer_outputs,brr)\n",
    "\n",
    "print(\"PRECISION:\",pre)\n",
    "print(\"RECALL:\",rec)\n",
    "print(\"FSCORE:\",fsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c07440e",
   "metadata": {},
   "source": [
    "## PRECISION RECALL AND FSCORE FOR INDICBERT MODEL MAX LENGTH =2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b932c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\",max_length=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50e1cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "brr = []\n",
    "\n",
    "for j in range(0, len(sentences)):\n",
    "    sentence = sentences[j][0]\n",
    "    tokens = tokenizer.encode(sentence)\n",
    "\n",
    "    arr = []\n",
    "    for token_id in tokens:\n",
    "        token = tokenizer.decode([token_id])  # Decode token ID to get the actual token\n",
    "        if token not in hindi_punctuation_symbols:\n",
    "            x = remove_hindi_symbol_prefix(token)\n",
    "            arr.append(x)\n",
    "    brr.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f951599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECISION: 0.011131725417439703\n",
      "RECALL: 0.05555555555555555\n",
      "FSCORE: 0.01854714064914992\n"
     ]
    }
   ],
   "source": [
    "pre,rec,fsc=calculate_metrics(tokenizer_outputs,brr)\n",
    "\n",
    "print(\"PRECISION:\",pre)\n",
    "print(\"RECALL:\",rec)\n",
    "print(\"FSCORE:\",fsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc300b87",
   "metadata": {},
   "source": [
    "## PRECISION RECALL AND FSCORE FOR WHITE SPACE TOKENIZER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6e9aea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "brr = []\n",
    "\n",
    "for j in range(0, len(sentences)):\n",
    "    sentence = sentences[j][0]\n",
    "    tokens = sentence.split()\n",
    "\n",
    "    arr = []\n",
    "    for token in tokens:\n",
    "        if token not in hindi_punctuation_symbols:\n",
    "            x = remove_hindi_symbol_prefix(token)\n",
    "            arr.append(x)\n",
    "    brr.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9008f983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECISION: 0.045454545454545456\n",
      "RECALL: 0.14084507042253522\n",
      "FSCORE: 0.06872852233676977\n"
     ]
    }
   ],
   "source": [
    "pre,rec,fsc=calculate_metrics(tokenizer_outputs,brr)\n",
    "\n",
    "print(\"PRECISION:\",pre)\n",
    "print(\"RECALL:\",rec)\n",
    "print(\"FSCORE:\",fsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d931e3cc",
   "metadata": {},
   "source": [
    "# PRECISION RECALL AND FSCORE FOR MBERT MODEL MAXLEN=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2ea2dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased',max_length=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87ae79d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "brr = []\n",
    "\n",
    "for j in range(0, len(sentences)):\n",
    "    sentence = sentences[j][0]\n",
    "    tokens = tokenizer.encode(sentence)\n",
    "\n",
    "    arr = []\n",
    "    for token_id in tokens:\n",
    "        token = tokenizer.decode([token_id])  # Decode token ID to get the actual token\n",
    "        if token not in hindi_punctuation_symbols:\n",
    "            x = remove_hindi_symbol_prefix(token)\n",
    "            arr.append(x)\n",
    "    brr.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea766b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECISION: 0.0195160031225605\n",
      "RECALL: 0.11627906976744186\n",
      "FSCORE: 0.03342245989304813\n"
     ]
    }
   ],
   "source": [
    "pre,rec,fsc=calculate_metrics(tokenizer_outputs,brr)\n",
    "\n",
    "print(\"PRECISION:\",pre)\n",
    "print(\"RECALL:\",rec)\n",
    "print(\"FSCORE:\",fsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115800a8",
   "metadata": {},
   "source": [
    "# PRECISION RECALL AND FSCORE FOR MBERT MODEL MAXLEN=2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "caf12781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased',max_length=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "57b253dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "brr = []\n",
    "\n",
    "for j in range(0, len(sentences)):\n",
    "    sentence = sentences[j][0]\n",
    "    tokens = tokenizer.encode(sentence)\n",
    "\n",
    "    arr = []\n",
    "    for token_id in tokens:\n",
    "        token = tokenizer.decode([token_id])  # Decode token ID to get the actual token\n",
    "        if token not in hindi_punctuation_symbols:\n",
    "            x = remove_hindi_symbol_prefix(token)\n",
    "            arr.append(x)\n",
    "    brr.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45b0f405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECISION: 0.0195160031225605\n",
      "RECALL: 0.11627906976744186\n",
      "FSCORE: 0.03342245989304813\n"
     ]
    }
   ],
   "source": [
    "pre,rec,fsc=calculate_metrics(tokenizer_outputs,brr)\n",
    "\n",
    "print(\"PRECISION:\",pre)\n",
    "print(\"RECALL:\",rec)\n",
    "print(\"FSCORE:\",fsc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
